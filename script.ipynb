{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# CSV to Google Sheets Uploader\n",
    "\n",
    "This notebook reads CSV data and uploads it to Google Sheets using the same endpoint and data structure as your existing Node.js implementation.\n",
    "\n",
    "## Features:\n",
    "- Reads CSV files using pandas\n",
    "- Transforms data to match your User model structure\n",
    "- Handles date formatting\n",
    "- Uploads to Google Sheets via Apps Script endpoint\n",
    "- Comprehensive error handling and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0612ae63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.2-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\patil\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\patil\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\patil\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.2-cp313-cp313-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.0/11.0 MB 9.3 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.1/11.0 MB 11.6 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.2/11.0 MB 12.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.0/11.0 MB 9.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.8/11.0 MB 7.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.3/11.0 MB 7.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.6/11.0 MB 6.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.1/11.0 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.4/11.0 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.7/11.0 MB 4.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.9/11.0 MB 4.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.2/11.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.4/11.0 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.7/11.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.2/11.0 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.5/11.0 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.7/11.0 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 3.1 MB/s  0:00:03\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "\n",
      "   ---------------------------------------- 0/3 [pytz]\n",
      "   ---------------------------------------- 0/3 [pytz]\n",
      "   ------------- -------------------------- 1/3 [tzdata]\n",
      "   ------------- -------------------------- 1/3 [tzdata]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   ---------------------------------------- 3/3 [pandas]\n",
      "\n",
      "Successfully installed pandas-2.3.2 pytz-2025.2 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from typing import List, Dict, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "configuration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Target Sheet ID: 1ztK5yqeGP83c4db7kbWsi7jQvyQhug7fJb-UtrpBdWM\n",
      "📁 CSV File: pnb-metlife.users.csv\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Update these values as needed\n",
    "SHEET_URL = \"https://script.google.com/macros/s/AKfycbxM_OfnwX9wo3DqlIV_G5C0pD_InJSvSa7UcxqERazPjoldIooZDO9SfMJTUvxCssq7/exec\"\n",
    "SHEET_ID = \"1ztK5yqeGP83c4db7kbWsi7jQvyQhug7fJb-UtrpBdWM\"\n",
    "\n",
    "# CSV file path - Update this to your CSV file location\n",
    "CSV_FILE_PATH = \"pnb-metlife.users.csv\"  # Replace with your actual CSV file path\n",
    "\n",
    "print(f\"📊 Target Sheet ID: {SHEET_ID}\")\n",
    "print(f\"📁 CSV File: {CSV_FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "data_processing_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_dataframe(df: pd.DataFrame) -> List[List[str]]:\n",
    "    \"\"\"Process DataFrame and return formatted values for sheets\"\"\"\n",
    "    # Define the expected column order based on your User model\n",
    "    expected_columns = [\n",
    "        '_id', 'name', 'mobile', 'consent', 'channel', 'employeeCode', \n",
    "        'buisnessCode', 'city', 'age', 'monthlyExpense', 'retirementAge', \n",
    "        'futureValue', 'retirementCorpus', 'yearlyInvestment', 'createdAt', \n",
    "        'updatedAt', 'utm', 'utm_source', 'utm_campaign'\n",
    "    ]\n",
    "    \n",
    "    # Create a new DataFrame with the expected structure\n",
    "    processed_df = pd.DataFrame()\n",
    "    \n",
    "    values = processed_df.values.tolist()\n",
    "    \n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "google_sheets_client",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleSheetsClient:\n",
    "    \"\"\"Client for interacting with Google Sheets via Apps Script endpoint\"\"\"\n",
    "    \n",
    "    def __init__(self, sheet_url: str, sheet_id: str):\n",
    "        self.sheet_url = sheet_url\n",
    "        self.sheet_id = sheet_id\n",
    "        self.session = requests.Session()  # <-- create a session\n",
    "    \n",
    "    def upload_data(self, data: List[List[str]]) -> bool:\n",
    "        \"\"\"Upload data to Google Sheets\"\"\"\n",
    "        try:\n",
    "            payload = {\n",
    "                \"mode\": \"append\",\n",
    "                \"data\": data,\n",
    "                \"sheetId\": self.sheet_id\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"Uploading {len(data)} rows to Google Sheets\")\n",
    "            logger.info(f\"Payload preview: {payload['data'][:2] if len(payload['data']) > 0 else 'No data'}\")\n",
    "            \n",
    "            response = self.session.post(\n",
    "                self.sheet_url,\n",
    "                json=payload,\n",
    "                timeout=30\n",
    "            )\n",
    "            \n",
    "            response.raise_for_status()\n",
    "            \n",
    "            logger.info(\"✅ Data successfully uploaded to Google Sheets\")\n",
    "            logger.info(f\"Response: {response.text}\")\n",
    "            return True\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"❌ Failed to upload data: {e}\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Unexpected error during upload: {e}\")\n",
    "            return False\n",
    "\n",
    "# Initialize the client\n",
    "sheets_client = GoogleSheetsClient(SHEET_URL, SHEET_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "read_and_process_csv",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 04:39:44,748 - INFO - 📖 Reading CSV file: pnb-metlife.users.csv\n",
      "2025-08-22 04:39:44,756 - INFO - 🔄 Processing data for Google Sheets...\n",
      "2025-08-22 04:39:44,761 - INFO - Uploading 542 rows to Google Sheets\n",
      "2025-08-22 04:39:44,762 - INFO - Payload preview: [['68a7066dc3e5abf79d0452ae', 'Zeeshan Mohammad', 7899766044, True, '', 56.0, 'Bengaluru', '', 'fire_aug_25 ', '2025-08-21T11:43:41.116Z', '2025-08-21T11:43:47.009Z', 0, 31.0, 60000.0, 'social_media', '', 3090146.92, 104323334.32, 1644449.23], ['68a7069dc3e5abf79d0452b3', 'Raj', 7899768250, True, '', 57.0, 'Bangalore', '', 'fire_aug_25 ', '2025-08-21T11:44:29.167Z', '2025-08-21T11:44:34.580Z', 0, 34.0, 60000.0, 'social_media', '', 2750219.76, 84997336.22, 1524623.69]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 CSV loaded successfully!\n",
      "📈 Total rows: 542\n",
      "🏷️  Columns: ['_id', 'name', 'mobile', 'consent', 'channel', 'city', 'createdAt', 'updatedAt', 'age', 'monthlyExpense', 'retirementAge', 'futureValue', 'retirementCorpus', 'yearlyInvestment', 'employeeCode', 'utm', 'utm_source', 'utm_campaign', 'buisnessCode']\n",
      "\n",
      "🔍 First few rows:\n",
      "['68a7066dc3e5abf79d0452ae', 'Zeeshan Mohammad', 7899766044, True, '', 56.0, 'Bengaluru', '', 'fire_aug_25 ', '2025-08-21T11:43:41.116Z', '2025-08-21T11:43:47.009Z', 0, 31.0, 60000.0, 'social_media', '', 3090146.92, 104323334.32, 1644449.23]\n",
      "✅ Data processed successfully!\n",
      "📤 Ready to upload 542 rows\n",
      "📤 Uploading rows 0–999 (542 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 04:40:15,199 - ERROR - ❌ Failed to upload data: HTTPSConnectionPool(host='script.google.com', port=443): Read timed out. (read timeout=30)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to upload batch 0–999\n"
     ]
    }
   ],
   "source": [
    "# Read and process CSV data\n",
    "try:\n",
    "    # Read CSV file\n",
    "    logger.info(f\"📖 Reading CSV file: {CSV_FILE_PATH}\")\n",
    "    df = pd.read_csv(CSV_FILE_PATH)\n",
    "    \n",
    "    df.drop(columns=['__v'], inplace=True)\n",
    "    # Display basic info about the data\n",
    "    print(f\"📊 CSV loaded successfully!\")\n",
    "    print(f\"📈 Total rows: {len(df)}\")\n",
    "    print(f\"🏷️  Columns: {list(df.columns)}\")\n",
    "    print(\"\\n🔍 First few rows:\")\n",
    "\n",
    "    \n",
    "    # display(df.head())\n",
    "    # Process the data\n",
    "    logger.info(\"🔄 Processing data for Google Sheets...\")\n",
    "\n",
    "    df = df.fillna(\"\") \n",
    "\n",
    "    df = df.reindex(columns=[\n",
    "        '_id', 'name', 'mobile', 'consent', 'channel', 'employeeCode', \n",
    "        'buisnessCode', 'city', 'age', 'monthlyExpense', 'retirementAge', \n",
    "        'futureValue', 'retirementCorpus', 'yearlyInvestment', 'createdAt', \n",
    "        'updatedAt', 'utm', 'utm_source', 'utm_campaign'\n",
    "    ])\n",
    "\n",
    "    if 'mobile' in df.columns:\n",
    "        df['mobile'] = pd.to_numeric(df['mobile'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "    processed_data = df.values.tolist()\n",
    "\n",
    "    print(processed_data[0])\n",
    "    \n",
    "    print(f\"✅ Data processed successfully!\")\n",
    "    print(f\"📤 Ready to upload {len(processed_data)} rows\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ CSV file not found: {CSV_FILE_PATH}\")\n",
    "    print(\"Please update the CSV_FILE_PATH variable with the correct path to your CSV file.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error reading CSV: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 500\n",
    "\n",
    "def chunked_upload(client, data, batch_size=BATCH_SIZE):\n",
    "    total = len(data)\n",
    "    for start in range(0, total, batch_size):\n",
    "        end = start + batch_size\n",
    "        batch = data[start:end]\n",
    "        print(f\"📤 Uploading rows {start}–{end-1} ({len(batch)} rows)\")\n",
    "        \n",
    "        success = client.upload_data(batch)\n",
    "        if not success:\n",
    "            print(f\"❌ Failed to upload batch {start}–{end-1}\")\n",
    "            break\n",
    "\n",
    "\n",
    "# processed_data = df.values.tolist()  # already prepared\n",
    "chunked_upload(sheets_client, processed_data, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a55b3e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9826\n"
     ]
    }
   ],
   "source": [
    "print(len(processed_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "upload_to_sheets",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 19:03:02,256 - INFO - 🚀 Starting upload to Google Sheets...\n",
      "2025-08-21 19:03:02,257 - INFO - Uploading 9826 rows to Google Sheets\n",
      "2025-08-21 19:03:02,257 - INFO - Payload preview: [['688f97f9af6fe3b64062e328', 'kfekf', 97, True, 'social_media', '', '', 'ss', 60.0, 170000.0, 43.0, 757583.41, 68856588.33, 225872.08, '2025-08-03T17:10:17.787Z', '2025-08-03T17:10:25.149Z', '', '', ''], ['68904932af6fe3b64062e33f', 'Nishant Neeraj', 9871077114, True, 'social_media', '', '', 'Delhi', 50.0, 100000.0, 65.0, 2875869.83, 37906250.53, 2160056.0, '2025-08-04T05:46:26.043Z', '2025-08-04T05:46:44.093Z', '', '', '']]\n",
      "2025-08-21 19:03:33,169 - ERROR - ❌ Failed to upload data: HTTPSConnectionPool(host='script.google.com', port=443): Read timed out. (read timeout=30)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💥 Upload failed! Check the logs above for details. False\n"
     ]
    }
   ],
   "source": [
    "# Upload data to Google Sheets\n",
    "if 'processed_data' in locals() and processed_data:\n",
    "    logger.info(\"🚀 Starting upload to Google Sheets...\")\n",
    "    \n",
    "    success = sheets_client.upload_data(processed_data)\n",
    "    \n",
    "    if success:\n",
    "        print(\"🎉 Upload completed successfully!\")\n",
    "        print(f\"📊 {len(processed_data)} rows uploaded to Google Sheets\")\n",
    "    else:\n",
    "        print(\"💥 Upload failed! Check the logs above for details.\", success)\n",
    "else:\n",
    "    print(\"⚠️  No processed data available. Please run the previous cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5be82bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 20:00:01,821 - INFO - Uploading 500 rows to Google Sheets\n",
      "2025-08-21 20:00:01,821 - INFO - Payload preview: [['689565345b559c022b76aab8', 'Nishkam', 9897320666, True, 'pnb', 8038685.0, '', '', 35.0, 35000.0, 60.0, 1802585.7, 41956930.52, 1125462.76, '2025-08-08T02:47:16.633Z', '2025-08-08T02:47:38.988Z', '', '', ''], ['689565cb17c94b36ba15df84', 'Sumit Kumar', 7037531182, True, 'pnb', 8036334.0, '', '', 28.0, 50000.0, 60.0, 3872032.01, 90125300.44, 2417542.66, '2025-08-08T02:49:47.984Z', '2025-08-08T02:50:00.141Z', '', '', '']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📤 Uploading rows 0–499 (500 rows)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 20:00:32,116 - ERROR - ❌ Failed to upload data: HTTPSConnectionPool(host='script.google.com', port=443): Read timed out. (read timeout=30)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to upload batch 0–499\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 500\n",
    "\n",
    "def chunked_upload(client, data, batch_size=BATCH_SIZE):\n",
    "    total = len(data)\n",
    "    for start in range(0, total, batch_size):\n",
    "        end = start + batch_size\n",
    "        batch = data[start:end]\n",
    "        print(f\"📤 Uploading rows {start}–{end-1} ({len(batch)} rows)\")\n",
    "        \n",
    "        success = client.upload_data(batch)\n",
    "        if not success:\n",
    "            print(f\"❌ Failed to upload batch {start}–{end-1}\")\n",
    "            break\n",
    "\n",
    "\n",
    "# processed_data = df.values.tolist()  # already prepared\n",
    "chunked_upload(sheets_client, processed_data, batch_size=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usage_instructions",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "1. **Update Configuration**: Modify the `CSV_FILE_PATH` variable in the configuration cell to point to your CSV file\n",
    "2. **Run All Cells**: Execute all cells in order (Shift + Enter or Run All)\n",
    "3. **Check Results**: Monitor the output and logs for success/failure status\n",
    "\n",
    "## Expected CSV Format\n",
    "\n",
    "Your CSV should have columns matching your User model structure:\n",
    "- `_id`: User ID\n",
    "- `name`: User name\n",
    "- `mobile`: Mobile number\n",
    "- `consent`: Consent status\n",
    "- `channel`: Channel (agency, pnb, jkb, kbl, psf, social_media)\n",
    "- `employeeCode`: Employee code\n",
    "- `buisnessCode`: Business code\n",
    "- `city`: City\n",
    "- `age`: Age\n",
    "- `monthlyExpense`: Monthly expense\n",
    "- `retirementAge`: Retirement age\n",
    "- `futureValue`: Future value\n",
    "- `retirementCorpus`: Retirement corpus\n",
    "- `yearlyInvestment`: Yearly investment\n",
    "- `createdAt`: Creation timestamp (ISO format)\n",
    "- `updatedAt`: Update timestamp (ISO format)\n",
    "- `utm`: UTM parameter\n",
    "- `utm_source`: UTM source\n",
    "- `utm_campaign`: UTM campaign\n",
    "\n",
    "## Notes\n",
    "\n",
    "- Missing columns will be filled with empty strings\n",
    "- Dates are automatically formatted to DD-MM-YYYY HH:MM:SS format\n",
    "- The script handles null/empty values gracefully\n",
    "- All data is uploaded in append mode to your existing sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample_data_creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Create a sample CSV file for testing\n",
    "def create_sample_csv():\n",
    "    \"\"\"Create a sample CSV file for testing purposes\"\"\"\n",
    "    sample_data = {\n",
    "        '_id': ['507f1f77bcf86cd799439011', '507f1f77bcf86cd799439012', '507f1f77bcf86cd799439013'],\n",
    "        'name': ['John Doe', 'Jane Smith', 'Bob Johnson'],\n",
    "        'mobile': [9876543210, 9876543211, 9876543212],\n",
    "        'consent': [True, True, False],\n",
    "        'channel': ['agency', 'pnb', 'social_media'],\n",
    "        'employeeCode': [1234567, 1234568, None],\n",
    "        'buisnessCode': [87654321, 87654322, None],\n",
    "        'city': ['Mumbai', 'Delhi', 'Bangalore'],\n",
    "        'age': [30, 28, 35],\n",
    "        'monthlyExpense': [50000, 45000, 60000],\n",
    "        'retirementAge': [60, 58, 65],\n",
    "        'futureValue': [1000000, 800000, 1200000],\n",
    "        'retirementCorpus': [5000000, 4000000, 6000000],\n",
    "        'yearlyInvestment': [100000, 90000, 120000],\n",
    "        'createdAt': ['2024-01-15T10:30:00Z', '2024-01-15T11:00:00Z', '2024-01-15T11:30:00Z'],\n",
    "        'updatedAt': ['2024-01-15T10:30:00Z', '2024-01-15T11:00:00Z', '2024-01-15T11:30:00Z'],\n",
    "        'utm': ['google', 'facebook', 'instagram'],\n",
    "        'utm_source': ['google_ads', 'facebook_ads', 'instagram_ads'],\n",
    "        'utm_campaign': ['Fire Campaign', 'Fire Campaign', 'Fire Campaign']\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(sample_data)\n",
    "    df.to_csv('sample_data.csv', index=False)\n",
    "    print(\"✅ Sample CSV file 'sample_data.csv' created successfully!\")\n",
    "    print(\"You can now use this file for testing by setting CSV_FILE_PATH = 'sample_data.csv'\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Uncomment the line below to create a sample CSV file\n",
    "# sample_df = create_sample_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fix_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the missing helper functions and fix the process_dataframe function\\n\n",
    "def format_date(iso_date):\\n\n",
    "    \"\"\"Transform ISO date to DD-MM-YYYY HH:MM:SS format\"\"\"\\n\n",
    "    if pd.isna(iso_date) or iso_date == \"null\" or iso_date == \"\":\\n\n",
    "        return \"\"\\n\n",
    "    \\n\n",
    "    try:\\n\n",
    "        if isinstance(iso_date, str):\\n\n",
    "            # Handle ISO string format\\n\n",
    "            date_obj = pd.to_datetime(iso_date)\\n\n",
    "        else:\\n\n",
    "            date_obj = pd.to_datetime(iso_date)\\n\n",
    "        \\n\n",
    "        return date_obj.strftime(\"%d-%m-%Y %H:%M:%S\")\\n\n",
    "    except:\\n\n",
    "        logger.warning(f\"Could not parse date: {iso_date}\")\\n\n",
    "        return \"\"\\n\n",
    "\\n\n",
    "def clean_value(value):\\n\n",
    "    \"\"\"Clean and format values for sheets\"\"\"\\n\n",
    "    if pd.isna(value) or value == \"null\" or value == \"\":\\n\n",
    "        return \"\"\\n\n",
    "    return str(value)\\n\n",
    "\\n\n",
    "# Fix the process_dataframe function\\n\n",
    "def process_dataframe(df: pd.DataFrame) -> List[List[str]]:\\n\n",
    "    \"\"\"Process DataFrame and return formatted values for sheets\"\"\"\\n\n",
    "    # Define the expected column order based on your User model\\n\n",
    "    expected_columns = [\\n\n",
    "        '_id', 'name', 'mobile', 'consent', 'channel', 'employeeCode', \\n\n",
    "        'buisnessCode', 'city', 'age', 'monthlyExpense', 'retirementAge', \\n\n",
    "        'futureValue', 'retirementCorpus', 'yearlyInvestment', 'createdAt', \\n\n",
    "        'updatedAt', 'utm', 'utm_source', 'utm_campaign'\\n\n",
    "    ]\\n\n",
    "    \\n\n",
    "    # Create a new DataFrame with the expected structure\\n\n",
    "    processed_df = pd.DataFrame()\\n\n",
    "    \\n\n",
    "    for col in expected_columns:\\n\n",
    "        if col in df.columns:\\n\n",
    "            if col in ['createdAt', 'updatedAt']:\\n\n",
    "                processed_df[col] = df[col].apply(format_date)\\n\n",
    "            else:\\n\n",
    "                processed_df[col] = df[col].apply(clean_value)\\n\n",
    "        else:\\n\n",
    "            # Add missing columns with empty values\\n\n",
    "            processed_df[col] = \"\"\\n\n",
    "    \\n\n",
    "    # Convert to list of lists for sheets\\n\n",
    "    values = processed_df.values.tolist()\\n\n",
    "    \\n\n",
    "    return values\\n\n",
    "\\n\n",
    "print(\"✅ Helper functions added successfully!\")\\n\n",
    "print(\"Now you can run the data processing cell again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fix_sheets_client",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the GoogleSheetsClient class\\n\n",
    "class GoogleSheetsClient:\\n\n",
    "    \"\"\"Client for interacting with Google Sheets via Apps Script endpoint\"\"\"\\n\n",
    "    \\n\n",
    "    def __init__(self, sheet_url: str, sheet_id: str):\\n\n",
    "        self.sheet_url = sheet_url\\n\n",
    "        self.sheet_id = sheet_id\\n\n",
    "        self.session = requests.Session()\\n\n",
    "        self.session.headers.update({\\n\n",
    "            'Content-Type': 'application/json',\\n\n",
    "            'User-Agent': 'CSV-to-Sheets-Uploader/1.0'\\n\n",
    "        })\\n\n",
    "    \\n\n",
    "    def upload_data(self, data: List[List[str]]) -> bool:\\n\n",
    "        \"\"\"Upload data to Google Sheets\"\"\"\\n\n",
    "        try:\\n\n",
    "            payload = {\\n\n",
    "                \"mode\": \"append\",\\n\n",
    "                \"data\": data,\\n\n",
    "                \"sheetId\": self.sheet_id\\n\n",
    "            }\\n\n",
    "            \\n\n",
    "            logger.info(f\"Uploading {len(data)} rows to Google Sheets\")\\n\n",
    "            logger.info(f\"Payload preview: {payload['data'][:2] if len(payload['data']) > 0 else 'No data'}\")\\n\n",
    "            \\n\n",
    "            response = self.session.post(\\n\n",
    "                self.sheet_url,\\n\n",
    "                json=payload,\\n\n",
    "                timeout=30\\n\n",
    "            )\\n\n",
    "            \\n\n",
    "            response.raise_for_status()\\n\n",
    "            \\n\n",
    "            logger.info(\"✅ Data successfully uploaded to Google Sheets\")\\n\n",
    "            logger.info(f\"Response: {response.text}\")\\n\n",
    "            return True\\n\n",
    "            \\n\n",
    "        except requests.exceptions.RequestException as e:\\n\n",
    "            logger.error(f\"❌ Failed to upload data: {e}\")\\n\n",
    "            return False\\n\n",
    "        except Exception as e:\\n\n",
    "            logger.error(f\"❌ Unexpected error during upload: {e}\")\\n\n",
    "            return False\\n\n",
    "\\n\n",
    "# Reinitialize the client with the fixed class\\n\n",
    "sheets_client = GoogleSheetsClient(SHEET_URL, SHEET_ID)\\n\n",
    "print(\"✅ GoogleSheetsClient fixed and reinitialized!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
